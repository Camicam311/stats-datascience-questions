{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q (Google, Hard):\n",
    "What does it mean for an estimator to be unbiased? What about consistent? Give examples of an unbiased but not consistent estimator, as well as a biased but consistent estimator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A:\n",
    "We can think of Estimator Bias as the following:\n",
    "- Archer: estimator $\\hat\\theta(X_1,...,X_n)$\n",
    "- Bow and arrow: data $x_1,...,x_n$\n",
    "- Arrow in target: estimate, $\\hat\\theta(x_1,...,x_n)$\n",
    "- Bullseye: Parameter of interest $\\theta$\n",
    "\n",
    "**Bias**, $B$, of an estimator, $\\hat\\theta$, is $B=E(\\hat\\theta)-\\theta$.\n",
    "\n",
    "An estimator is **unbiased** if and only if $B=0$ or equivalently, $E(\\hat\\theta)=\\theta$.\n",
    "\n",
    "An estimator is **consistent** if it converges in probability to the true value of the parameter.\n",
    "\n",
    "<br/><br/>\n",
    "**Bias & Consistent Estimator:**\n",
    "\n",
    "An example of an estimator that is *consistent* and *biased* is the sample variance:\n",
    "$$S^2_n=\\frac{1}{n}\\sum^n_{i=1}(X_i-\\bar X)^2$$\n",
    "It is easy to show that $E(S^2_n)=\\frac{n-1}{n}\\sigma^2$. Hence the estimator is *biased*.\n",
    "\n",
    "Assuming finite variance $\\sigma^2$, we see that bias converges to zero as $n$ goes to infinity because\n",
    "$$E(S^2_n)-\\sigma^2 = -\\frac{1}{n}\\sigma^2$$\n",
    "It can also be shown that the variance of the estimator tends to zero and so the estimator converges in mean-square. Hence, it is also convergent in probability.\n",
    "\n",
    "<br/><br/>\n",
    "**Unbiased & Not consistent estimator:**\n",
    "\n",
    "Consider an iid sample, ${x_1,...,x_n}$, we can use $T_n(X)=x_n$ as an estimator of the mean $E(x)$. Note the sampling distribution of $T_n$ is the same as the underlying distribution (for any $n$, as it ignores all points but the last), so $E[T_n(X)]=E(X)$ and it is unbiased, but it does not converge to any value.\n",
    "> Suppose your sample was drawn from a distribution with mean $\\mu$ and variance $\\sigma^2$. Your estimator $\\tilde x=x_1$ is unbiased as $E(\\tilde x) = E(x_1) = \\mu$ implies the expected value of the estimator equals the population mean. Your estimator is on the other hand *inconsistent*, since $\\tilde x$ is fixed at $x_1$ and will not change with the changing sample size, i.e. will not converge in probability to $\\mu$.\n",
    "\n",
    "However, if a sequence of estimators is unbiased and converges to a value, then it is consistent, as it must converge to the correct value.\n",
    "\n",
    "\n",
    "Source:\n",
    "- https://en.wikipedia.org/wiki/Consistent_estimator\n",
    "- https://stats.stackexchange.com/questions/174137/an-example-of-a-consistent-and-biased-estimator\n",
    "- https://math.stackexchange.com/questions/119461/problem-with-unbiased-but-not-consistent-estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q:\n",
    "Assume we have a classifier that produces a score between 0 and 1 for the probability of a particular loan \n",
    "application being fraudulent. In this scenario: \n",
    "\n",
    "a) what are false positives \n",
    "\n",
    "b) what are false negatives\n",
    "\n",
    "c) what are the trade-offs between them in terms of dollars and how should the model be weighted accordingly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A:\n",
    "Let's first review what <strong>False positive</strong> and <strong>False negative</strong> means.\n",
    "\n",
    "<strong>False positive</strong>: Also known as <strong>Type I</strong> error, is when you test yourself for pregnancy and get a positive result, but in reality you are not. <mark>In our scenario, a false positive means that the loan application is classified as fraudulent, but in reality it is not.</mark>\n",
    "\n",
    "<strong>False negative</strong>: Also known as <strong> Type II</strong> error, is when you get a negative result, but in reality you are pregnant. <mark>In our scenario, the classifier classifies the loan application as not fraudulent, but in reality it is actually fraudulent.</mark>\n",
    "\n",
    "<strong>Sensitivity (TPR/Recall)</strong>: is the ability of a classifier to detect whether a loan application is fraudulent.\n",
    "<br/><br/>\n",
    "$$Senitivity = \\frac{number\\,of\\,true\\,positives\\,(TP)}{number\\,of\\,true\\,positives\\,(TP) + number\\,of\\,false\\,negatives\\,(FN)}=\\frac{correctly\\,classified\\,fraudulent\\,applications}{total\\,number\\,of\\,fraudulent\\,applications}$$\n",
    "\n",
    "A negative test result for a classifier with high sensitivity is reliable since the classifier rarely misclassifies fraudulent applications. A positive test result for a classifier with high sensitivity is not neccessarily useful. If there is a classifier that always classifies applications positive 100% of the time, then the result will always be positive. However, it is important to note at this point that sensitivity by definition does not take account false positives. So in this sense, this classifier is useless.\n",
    "\n",
    "<strong>Specificity</strong>: is the ability of the classifier to correctly detect whether a loan application is not fraudulent.\n",
    "<br/><br/>\n",
    "$$Specificity = \\frac{number\\,of\\,true\\,negatives\\,(TN)}{number\\,of\\,true\\,negatives\\,(TN) + number\\,of\\,false\\,positives\\,(FP)} = \\frac{true\\,applications\\,classified\\,as\\,not\\,fraudulent}{total\\,number\\,of\\,true\\,applications}$$\n",
    "\n",
    "A positive result in a test with high specificity is useful for ruling in as fraudulent. The test rarely gives positive results for applications that are not fraudulent. A positive results means high probability of the presence of fraudulent application.\n",
    "\n",
    "<strong>False Positive Rate (TPR)</strong>:$1-\\frac{FP}{FP+TN}$\n",
    "\n",
    "<strong>AUC-ROC Curve</strong>: is a performance measurement for classification problem at various threshold settings. ROC (Receiver Operating Characteristics) is a probability curve and AUC (Area Under the Curve) represent degree or measure of separability. It tells us how much model is capable of distinguishing between classes. The higher the AUC, the better the model is at predicting 0's as 0's and 1's as 1's. By analogy, the higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n",
    "- Speculating model performance: A perfect model has AUC close to 1, on the other hand, a poor model has AUC near 0 (this means that it is classifiying 0s as 1s and 1s and 0s). Lastly, when AUC is 0.5, the model has no class separation capacity. \n",
    "<br/><br/>\n",
    "<strong><center>'An Ideal Model (AUC=1)'</center></strong>\n",
    "![img](assets/auc_1.png)\n",
    "When AUC = 1, the model is perfectly able to distinguish betwween positive class and negative class.\n",
    "<br/><br/>\n",
    "<strong><center>'AUC = 0.7'</center></strong>\n",
    "![img](assets/auc_0.5.png)\n",
    "When AUC = 0.7, then there is a 70% chance that model will be able to distinguish between positive class and negative class.\n",
    "<br/><br/>\n",
    "<strong><center>'AUC = 0.5'</center></strong>\n",
    "![img](assets/auc_0.5_real.png)\n",
    "When AUC = 0.5 model is unable to distinguish between positive class and negative class.\n",
    "<br/><br/>\n",
    "<strong><center>'AUC = 0'</center></strong>\n",
    "![img](assets/auc_0.png)\n",
    "When AUC = 0 model is reciprocating classes.\n",
    "\n",
    "<strong>Relationship between Sensitivity, Specifity, FPR and Threshold</strong>: Sensitivity and Specifity are inversely proportional to each other. So when we increase sensitivity, specificity decreases and vice-versa. \n",
    "<br/><br/>\n",
    "<center>Sensitivity⬆️, Specificity⬇️ and Sensitivity⬇️, Specificity⬆️</center>\n",
    "<br/><br/>\n",
    "Similarly, when we increase the threshold, we get more negative values, thus we get higher specificity and lower sensitivity. Since FPR is 1-specificity, so when we increase TPR, FPR also increases and vice-versa.\n",
    "<br/><br/>\n",
    "<center>TPR⬆️, FPR⬆️ and TPR⬇️, FPR⬇️</center>\n",
    "\n",
    "\n",
    "Source:\n",
    "- https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    "- https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Q:\n",
    "Given that $X$~$Unif(0,1)$ and $YUnif(0,1)$, what is the expected value of the mimimum of X and Y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A:\n",
    "#### Motivation:\n",
    "Let $Z=min(X,Y)$. \n",
    "\n",
    "Then:\n",
    "$$1 - F_z(z) = P(min(X,Y) > z) = P(X>z,Y>z)$$\n",
    "\n",
    "$$= 1-P(X\\leq z) - P(Y \\leq z) + P(X\\leq z, Y \\leq z)$$\n",
    "\n",
    "$$= F_x(z) + F_y(z) - F_x(z)F_y(z) = F_{min}(X,Y)$$\n",
    "\n",
    "where last inequality is by the <strong>inclusion-exclusion principle</strong>: $A\\cup B = A + B - A\\cap B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution:\n",
    "Note: $F(y) = 1 - P(Y>y) = 1 - P(min(X_1,...X_n)>y)$ where $min(X_1,...X_n) > y$ whenever $X_i > y$ for all $i$.\n",
    "\n",
    "Hence, $F(y)=1-P(X_1>y)P(X_2>y)...P(X_n>y)= 1 - P(X_1>y)^n$.\n",
    "\n",
    "And since $P(X>y)=\\frac{b-y}{b-a}$ by definition (note, $P(X<y)=\\frac{y-a}{b-a}$), we have that $1-P(X_1>y)^n = 1-(\\frac{b-y}{b-a})^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivative w.r.t. $y$, we get $f(y)=\\frac{n}{b-a}(\\frac{b-y}{b-a})^{n-1}$\n",
    "\n",
    "Since $E(y) = \\int_{-\\infty}^{\\infty} yf(y)dy$, we get that $E(Y)=\\frac{b+na}{n+1}=\\frac{1}{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another solution:\n",
    "Let $X_1,...X_n$ be iid R.V. from uniform distribution and let $Z_n = min(X_1,...,X_n)$. Then $E[Z_n]=\\int_0^1P(X>x)^ndx=\\int_0^1(1-x)^ndx=\\frac{1}{n+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://tsourakakis.com/2015/12/14/expectation-of-minimum-of-n-i-i-d-uniform-random-variables/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
